{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff52e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from rouge_metric import get_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ad4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT AND OUTPUT FILES FOR MINERVA2B-base\n",
    "in_path_file = \"../results/minerva3B-base/minerva3B-base-out_with_human_ann.json\"\n",
    "out_path_file = \"../results/minerva3B-base/minerva3B_final_scores.json\"\n",
    "\n",
    "## INPUT AND OUTPUT FILE FOR mt5-base\n",
    "# in_path_file = \"../results/mt5-base/mt5-base_with_human_ann.json\"\n",
    "# out_path_file = \"../results/mt5-base/mt5-base_final_scores.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b8a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/minerva3B-base/judge_prometheus_for_minerva3B-base.json\") as f:\n",
    "    prometheus_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50df9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(in_path_file) as f:\n",
    "    model_outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/minerva3B-base/judge_gemini_for_minerva3B-base.json\", \"r\", encoding='utf-8') as f:\n",
    "    gemini_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9428b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 2, 5, 3, 2, 4, 5, 5, 2, 1, 3, 3, 2, 3, 5, 4, 3, 3, 2, 5, 5, 4, 5, 2, 3, 5, 5, 5, 4, 5, 5, 1, 5, 5, 3, 2, 4, 2, 5, 5, 5, 5, 5, 2, 3, 5, 3, 5, 3, 3, 2, 5, 3, 2, 4, 4, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 4, 5, 5, 2, 2, 5, 5, 5, 5, 3, 5, 5, 2, 2, 3, 2, 3, 5, 5, 2, 4, 4, 1, 5, 5, 5, 5, 2, 2, 2, 5, 2, 2, 5, 5, 5, 4, 5, 3, 4, 4, 5, 1, 5, 5, 5, 3, 5, 1, 5, 4, 4, 3, 2, 1, 3]\n",
      "125\n",
      "125\n",
      "[Main]: computing cohen's kappa coefficient for rouge score\n",
      "[Main]: computing cohen's coefficient for llm score\n",
      "[Main]: finished computed scores {'human': 0.7535999999999999, 'score_rouge1': 0.8944000000000001, 'score_rouge2': 0.8160000000000001, 'score_rougeL': 0.8944000000000001, 'cohen_rouge1': 0.22418657137483544, 'cohen_rouge2': 0.18136570031435573, 'cohen_rougeL': 0.22418657137483544, 'cohen_llm': -0.04379065928730941, 'cohen_prometheus': 0.14972955253237186, 'score_llm': 0.8400000000000001, 'score_prometheus': 0.5584}\n",
      "[Main]: writing in output the results\n"
     ]
    }
   ],
   "source": [
    "valuation_coefficient = 5\n",
    "\n",
    "human_annotations = [row['human_annotation'] for row in model_outputs]\n",
    "print(human_annotations)\n",
    "\n",
    "results = {'human': (sum(human_annotations)/len(human_annotations))/valuation_coefficient}\n",
    "\n",
    "rouge_results = get_rouge(in_path_file)\n",
    "\n",
    "assert(len(human_annotations) == len(rouge_results))\n",
    "print(len(human_annotations))\n",
    "print(len(gemini_results))\n",
    "assert(len(human_annotations) == len(gemini_results))\n",
    "\n",
    "# Computing Cohen's kappa\n",
    "rouge_1 = []\n",
    "rouge_2 = []\n",
    "rouge_L = []\n",
    "for r in rouge_results:\n",
    "    rouge_1.append(round(valuation_coefficient * r['rouge-1']['f']))\n",
    "    rouge_2.append(round(valuation_coefficient * r['rouge-2']['f']))\n",
    "    rouge_L.append(round(valuation_coefficient * r['rouge-l']['f']))\n",
    "\n",
    "print(\"[Main]: computing cohen's kappa coefficient for rouge score\")\n",
    "\n",
    "score1 = cohen_kappa_score(rouge_1, human_annotations)\n",
    "score2 = cohen_kappa_score(rouge_2, human_annotations)\n",
    "scoreL = cohen_kappa_score(rouge_L, human_annotations)\n",
    "results['score_rouge1'] = (sum(rouge_1)/len(rouge_1))/valuation_coefficient\n",
    "results['score_rouge2'] = (sum(rouge_2)/len(rouge_2))/valuation_coefficient\n",
    "results['score_rougeL'] = (sum(rouge_L)/len(rouge_L))/valuation_coefficient\n",
    "results['cohen_rouge1'] = score1\n",
    "results['cohen_rouge2'] = score2\n",
    "results['cohen_rougeL'] = scoreL\n",
    "\n",
    "\n",
    "print(\"[Main]: computing cohen's coefficient for llm score\")\n",
    "llm = []\n",
    "keys = list(gemini_results.keys())\n",
    "\n",
    "for k in keys:\n",
    "    llm.append(gemini_results[k])\n",
    "\n",
    "#for l in gemini_results:\n",
    "#    llm.append(l['score'])\n",
    "\n",
    "prometheus = []\n",
    "for k in keys:\n",
    "    prometheus.append(prometheus_results[k])\n",
    "\n",
    "score_llm = cohen_kappa_score(llm, human_annotations)\n",
    "score_prometheus = cohen_kappa_score(prometheus, human_annotations)\n",
    "results['cohen_llm'] = score_llm\n",
    "results['cohen_prometheus'] = score_prometheus\n",
    "results['score_llm'] = (sum(llm)/len(llm))/valuation_coefficient\n",
    "results['score_prometheus'] = (sum(prometheus)/len(prometheus))/valuation_coefficient\n",
    "\n",
    "print(\"[Main]: finished computed scores\", results)\n",
    "print(\"[Main]: writing in output the results\")\n",
    "with open(out_path_file, 'w') as out:\n",
    "    json.dump(results, out, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5020db5",
   "metadata": {},
   "source": [
    "### Plots to visualize and compare the different scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d250a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file with the final scores\n",
    "with open(\"../results/minerva3B-base/minerva3B_final_scores.json\") as f:\n",
    "    final_scores = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cacb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human': 0.7535999999999999, 'score_rouge1': 0.8944000000000001, 'score_rouge2': 0.8160000000000001, 'score_rougeL': 0.8944000000000001, 'cohen_rouge1': 0.22418657137483544, 'cohen_rouge2': 0.18136570031435573, 'cohen_rougeL': 0.22418657137483544, 'cohen_llm': -0.04379065928730941, 'cohen_prometheus': 0.14972955253237186, 'score_llm': 0.8400000000000001, 'score_prometheus': 0.5584}\n"
     ]
    }
   ],
   "source": [
    "print(final_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
