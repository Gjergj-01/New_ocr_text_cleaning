{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c8e077",
   "metadata": {},
   "source": [
    "## Fine-tune Minerva3B-base \n",
    "To fine-tune Minerva3B-base we make use of the **unsloth** library. \n",
    "**NOTE**: The notebook was prepared to run on Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcceb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import json\n",
    "from huggingface_hub import HfApi, login\n",
    "import torch\n",
    "import os\n",
    "from google.colab import drive\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85ba8c",
   "metadata": {},
   "source": [
    "Mount drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971cd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ca851",
   "metadata": {},
   "source": [
    "To import `Minerva3B-base` you'll need to generate the token and log in to hugging-face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f53de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09df997",
   "metadata": {},
   "source": [
    "#### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f738cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"sapienzanlp/Minerva-3B-base-v1.0\",\n",
    "    max_seq_length = 1024,\n",
    "    load_in_4bit = True,    # ‚Üê saves memory\n",
    "    load_in_8bit = False,\n",
    "    full_finetuning = False,   # we'll perform partial fine-tuning using PEFT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cd11a",
   "metadata": {},
   "source": [
    "#### PEFT\n",
    "**PEFT** (Parameter-Efficient Fine-Tuning) is a set of technqiues with aim of efficiently fine tuning large language models. The idea, proposed by PEFT methods is that of updating only a subset of parameters, instead of all model's parameters, which in many situations can be prohibitive.\n",
    "\n",
    "**Low-Rank Adaption (LoRA)** is one of the most popular PEFT methods. It consists in freezing the original model's pretrained weights, and then using two samll matrices (called **update matrices**) to adapt the new data while keeping the overall number of parameters low.\n",
    "Now we are going to add LoRA adapters.\n",
    "\n",
    "Practically this means that we update only a samll number of parameters (1% to 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85821481",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c06be",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "\n",
    "In the following we load and prepare the dataset using the alpaca format:\n",
    "```\n",
    "\"\"\"Di seguito viene fornita un'istruzione che descrive uno specifico task, seguita dall'input dello user. Scrivi una risposta che completa la richiesta.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84d428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f31cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(\"../datasets/t5-datasets/train\")\n",
    "test_dataset = load_from_disk(\"../datasets/t5-datasets/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a0c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ocr', 'clean'],\n",
      "    num_rows: 1804\n",
      "})\n",
      "Dataset({\n",
      "    features: ['ocr', 'clean'],\n",
      "    num_rows: 244\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Di seguito viene fornita un'istruzione che descrive uno specifico task, seguita dall'input dello user. Scrivi una risposta che completa la richiesta.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "EOS = tokenizer.eos_token   # end-of-sequence token, we must add it the end to stop generation\n",
    "\n",
    "def prepare_ocr_dataset(examples):\n",
    "    instruction = \"Correggi il testo ocr fornito in input dall'utente\"\n",
    "    ocr_samples = examples[\"ocr\"]\n",
    "    clean_samples = examples['clean']\n",
    "\n",
    "    texts = []\n",
    "    for ocr, clean in zip(ocr_samples, clean_samples):\n",
    "        text = alpaca_prompt.format(instruction, ocr, clean) + EOS\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_train_dataset = train_dataset.map(prepare_ocr_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650156b4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71506e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ocr = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 20,  # ~ 10-20% of the total steps\n",
    "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 200,\n",
    "        learning_rate = 5e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", # to report training stats on wnadb\n",
    "\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer_ocr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294e2e6",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For the inference we prepare the dataset using the alpaca format as before, but leaving blank the **Response** field to let the model generate the ansewr. So we have:\n",
    "```\n",
    "\"\"\"Di seguito viene fornita un'istruzione che descrive uno specifico task, seguita dall'input dello user. Scrivi una risposta che completa la richiesta.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response: \"\"\n",
    "\"\"\"\n",
    "```\n",
    "Then we write a simple regular expression to retireve the *input* and the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f885a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "ocr_samples = test_dataset['ocr']\n",
    "clean_samples = test_dataset['clean']\n",
    "\n",
    "pattern = r\"### Instruction:\\s*(.*?)\\s*### Input:\\s*(.*?)\\s*### Response:\\s*(.*?)</s>\"\n",
    "instruction = \"Correggi il testo ocr fornito in input dall'utente\"\n",
    "\n",
    "results = []\n",
    "i = 0\n",
    "for ocr, clean in zip(ocr_samples, clean_samples):\n",
    "    i += 1\n",
    "    inputs = tokenizer([\n",
    "        alpaca_prompt.format(\n",
    "            instruction,\n",
    "            ocr,\n",
    "            \"\", # we leave this blank for generation\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    answer = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    match = re.search(pattern, answer[0])\n",
    "    if match:\n",
    "        input = match.group(2).strip()\n",
    "        response = match.group(3).strip()\n",
    "\n",
    "        d = {}\n",
    "        d['in'] = input\n",
    "        d['hyp'] = response\n",
    "        d['ref'] = clean\n",
    "\n",
    "        results.append(d)\n",
    "\n",
    "        if i < 5:\n",
    "            print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165502b",
   "metadata": {},
   "source": [
    "Save the model's answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/minerva3B-base/minerva3B-base.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b15d7",
   "metadata": {},
   "source": [
    "### Push to Hugging Face"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
